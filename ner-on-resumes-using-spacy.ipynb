{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "challenging-greenhouse",
   "metadata": {
    "papermill": {
     "duration": 0.019553,
     "end_time": "2021-06-27T07:55:57.598270",
     "exception": false,
     "start_time": "2021-06-27T07:55:57.578717",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56cefdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from pprint import pprint\n",
    "import re \n",
    "import math\n",
    "import json\n",
    "import spacy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.util import filter_spans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd901c4",
   "metadata": {},
   "source": [
    "## Convert data from Dataturks to SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "christian-bradley",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-27T07:55:57.653403Z",
     "iopub.status.busy": "2021-06-27T07:55:57.652392Z",
     "iopub.status.idle": "2021-06-27T07:55:57.655353Z",
     "shell.execute_reply": "2021-06-27T07:55:57.654736Z",
     "shell.execute_reply.started": "2021-06-26T20:11:48.103154Z"
    },
    "papermill": {
     "duration": 0.037449,
     "end_time": "2021-06-27T07:55:57.655489",
     "exception": false,
     "start_time": "2021-06-27T07:55:57.618040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
    "    training_data = []\n",
    "    lines=[]\n",
    "    with open(dataturks_JSON_FilePath, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        text = data['content'].replace(\"\\n\", \" \")\n",
    "        entities = []\n",
    "        data_annotations = data['annotation']\n",
    "        if data_annotations is not None:\n",
    "            for annotation in data_annotations:\n",
    "                #only a single point in text annotation.\n",
    "                point = annotation['points'][0]\n",
    "                labels = annotation['label']\n",
    "                # handle both list of labels or a single label.\n",
    "                if not isinstance(labels, list):\n",
    "                    labels = [labels]\n",
    "\n",
    "                for label in labels:\n",
    "                    point_start = point['start']\n",
    "                    point_end = point['end']\n",
    "                    point_text = point['text']\n",
    "\n",
    "                    lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
    "                    rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
    "                    if lstrip_diff != 0:\n",
    "                        point_start = point_start + lstrip_diff\n",
    "                    if rstrip_diff != 0:\n",
    "                        point_end = point_end - rstrip_diff\n",
    "                    entities.append((point_start, point_end + 1 , label))\n",
    "        training_data.append((text, {\"entities\" : entities}))\n",
    "    return training_data\n",
    "\n",
    "def trim_entity_spans(data: list) -> list:\n",
    "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
    "\n",
    "    Args:\n",
    "        data (list): The data to be cleaned in spaCy JSON format.\n",
    "\n",
    "    Returns:\n",
    "        list: The cleaned data.\n",
    "    \"\"\"\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(\n",
    "                    text[valid_start]):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and invalid_span_tokens.match(\n",
    "                    text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            valid_entities.append([valid_start, valid_end, label])\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77e603b",
   "metadata": {},
   "source": [
    "## Analysis the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "editorial-messaging",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-27T07:55:57.699633Z",
     "iopub.status.busy": "2021-06-27T07:55:57.698934Z",
     "iopub.status.idle": "2021-06-27T07:55:57.776014Z",
     "shell.execute_reply": "2021-06-27T07:55:57.775417Z",
     "shell.execute_reply.started": "2021-06-26T20:11:48.1177Z"
    },
    "papermill": {
     "duration": 0.10081,
     "end_time": "2021-06-27T07:55:57.776146",
     "exception": false,
     "start_time": "2021-06-27T07:55:57.675336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abhishek Jha Application Development Associate - Accenture  Bengaluru, '\n",
      " 'Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a  '\n",
      " '• To work for an organization which provides me the opportunity to improve '\n",
      " \"my skills and knowledge for my individual and company's growth in best \"\n",
      " 'possible ways.  Willing to relocate to: Bangalore, Karnataka  WORK '\n",
      " 'EXPERIENCE  Application Development Associate  Accenture -  November 2017 to '\n",
      " 'Present  Role: Currently working on Chat-bot. Developing Backend Oracle '\n",
      " 'PeopleSoft Queries for the Bot which will be triggered based on given input. '\n",
      " 'Also, Training the bot for different possible utterances (Both positive and '\n",
      " 'negative), which will be given as input by the user.  EDUCATION  B.E in '\n",
      " 'Information science and engineering  B.v.b college of engineering and '\n",
      " 'technology -  Hubli, Karnataka  August 2013 to June 2017  12th in '\n",
      " 'Mathematics  Woodbine modern school  April 2011 to March 2013  10th  '\n",
      " 'Kendriya Vidyalaya  April 2001 to March 2011  SKILLS  C (Less than 1 year), '\n",
      " 'Database (Less than 1 year), Database Management (Less than 1 year), '\n",
      " 'Database Management System (Less than 1 year), Java (Less than 1 year)  '\n",
      " 'ADDITIONAL INFORMATION  Technical Skills  '\n",
      " 'https://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN   '\n",
      " '• Programming language: C, C++, Java • Oracle PeopleSoft • Internet Of '\n",
      " 'Things • Machine Learning • Database Management System • Computer Networks • '\n",
      " 'Operating System worked on: Linux, Windows, Mac  Non - Technical Skills  • '\n",
      " 'Honest and Hard-Working • Tolerant and Flexible to Different Situations • '\n",
      " 'Polite and Calm • Team-Player',\n",
      " {'entities': [[1296, 1622, 'Skills'],\n",
      "               [993, 1154, 'Skills'],\n",
      "               [939, 957, 'College Name'],\n",
      "               [883, 905, 'College Name'],\n",
      "               [856, 860, 'Graduation Year'],\n",
      "               [771, 814, 'College Name'],\n",
      "               [727, 769, 'Designation'],\n",
      "               [407, 416, 'Companies worked at'],\n",
      "               [372, 405, 'Designation'],\n",
      "               [95, 145, 'Email Address'],\n",
      "               [60, 69, 'Location'],\n",
      "               [49, 58, 'Companies worked at'],\n",
      "               [13, 46, 'Designation'],\n",
      "               [0, 12, 'Name']]}]\n"
     ]
    }
   ],
   "source": [
    "dataset_path='data/Entity Recognition in Resumes.json'\n",
    "data = trim_entity_spans(convert_dataturks_to_spacy(dataset_path))\n",
    "pprint(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd19fde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset:  220\n"
     ]
    }
   ],
   "source": [
    "print(\"Total dataset: \",len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2ca6249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample resume texts:\n",
      "('Abhishek Jha Application Development Associate - Accenture  Bengaluru, '\n",
      " 'Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a  '\n",
      " '• To work for an organization which provides me the opportunity to improve '\n",
      " \"my skills and knowledge for my individual and company's growth in best \"\n",
      " 'possible ways.  Willing to relocate to: Bangalore, Karnataka  WORK '\n",
      " 'EXPERIENCE  Application Development Associate  Accenture -  November 2017 to '\n",
      " 'Present  Role: Currently working on Chat-bot. Developing Backend Oracle '\n",
      " 'PeopleSoft Queries for the Bot which will be triggered based on given input. '\n",
      " 'Also, Training the bot for different possible utterances (Both positive and '\n",
      " 'negative), which will be given as input by the user.  EDUCATION  B.E in '\n",
      " 'Information science and engineering  B.v.b college of engineering and '\n",
      " 'technology -  Hubli, Karnataka  August 2013 to June 2017  12th in '\n",
      " 'Mathematics  Woodbine modern school  April 2011 to March 2013  10th  '\n",
      " 'Kendriya Vidyalaya  April 2001 to March 2011  SKILLS  C (Less than 1 year), '\n",
      " 'Database (Less than 1 year), Database Management (Less than 1 year), '\n",
      " 'Database Management System (Less than 1 year), Java (Less than 1 year)  '\n",
      " 'ADDITIONAL INFORMATION  Technical Skills  '\n",
      " 'https://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN   '\n",
      " '• Programming language: C, C++, Java • Oracle PeopleSoft • Internet Of '\n",
      " 'Things • Machine Learning • Database Management System • Computer Networks • '\n",
      " 'Operating System worked on: Linux, Windows, Mac  Non - Technical Skills  • '\n",
      " 'Honest and Hard-Working • Tolerant and Flexible to Different Situations • '\n",
      " 'Polite and Calm • Team-Player')\n"
     ]
    }
   ],
   "source": [
    "print('Sample resume texts:')\n",
    "pprint(data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "daf8e35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity with start and end position in the text:\n",
      "{'entities': [[1296, 1622, 'Skills'],\n",
      "              [993, 1154, 'Skills'],\n",
      "              [939, 957, 'College Name'],\n",
      "              [883, 905, 'College Name'],\n",
      "              [856, 860, 'Graduation Year'],\n",
      "              [771, 814, 'College Name'],\n",
      "              [727, 769, 'Designation'],\n",
      "              [407, 416, 'Companies worked at'],\n",
      "              [372, 405, 'Designation'],\n",
      "              [95, 145, 'Email Address'],\n",
      "              [60, 69, 'Location'],\n",
      "              [49, 58, 'Companies worked at'],\n",
      "              [13, 46, 'Designation'],\n",
      "              [0, 12, 'Name']]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Entity with start and end position in the text:\")\n",
    "pprint(data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "093d2867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person number:  1\n",
      "Skills\n",
      "Skills\n",
      "College Name\n",
      "College Name\n",
      "Graduation Year\n",
      "College Name\n",
      "Designation\n",
      "Companies worked at\n",
      "Designation\n",
      "Email Address\n",
      "Location\n",
      "Companies worked at\n",
      "Designation\n",
      "Name\n",
      "\n",
      "\n",
      "person number:  2\n",
      "Email Address\n",
      "Skills\n",
      "Graduation Year\n",
      "College Name\n",
      "Degree\n",
      "Graduation Year\n",
      "College Name\n",
      "Degree\n",
      "Email Address\n",
      "Location\n",
      "Name\n",
      "\n",
      "\n",
      "person number:  3\n",
      "Skills\n",
      "Skills\n",
      "Skills\n",
      "Skills\n",
      "Skills\n",
      "Skills\n",
      "Skills\n",
      "Skills\n",
      "Skills\n",
      "College Name\n",
      "Degree\n",
      "Location\n",
      "Companies worked at\n",
      "Designation\n",
      "Location\n",
      "Companies worked at\n",
      "Designation\n",
      "Email Address\n",
      "Location\n",
      "Name\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index,person in enumerate(data[:3]):\n",
    "    print(\"person number: \",index+1)\n",
    "    for entity in person[1]['entities']:\n",
    "        print(entity[2])\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-astronomy",
   "metadata": {
    "papermill": {
     "duration": 0.075142,
     "end_time": "2021-06-27T07:57:22.037446",
     "exception": false,
     "start_time": "2021-06-27T07:57:21.962304",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "together-deviation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-27T07:57:22.191471Z",
     "iopub.status.busy": "2021-06-27T07:57:22.190499Z",
     "iopub.status.idle": "2021-06-27T07:57:22.194256Z",
     "shell.execute_reply": "2021-06-27T07:57:22.193590Z",
     "shell.execute_reply.started": "2021-06-26T20:12:53.998793Z"
    },
    "papermill": {
     "duration": 0.083463,
     "end_time": "2021-06-27T07:57:22.194402",
     "exception": false,
     "start_time": "2021-06-27T07:57:22.110939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "def train_test_split(data, test_size, random_state):\n",
    "\n",
    "    random.Random(random_state).shuffle(data)\n",
    "    test_idx = len(data) - math.floor(test_size * len(data))\n",
    "    train_set = data[0: test_idx]\n",
    "    test_set = data[test_idx: ]\n",
    "\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "accessible-horizon",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-27T07:57:22.349522Z",
     "iopub.status.busy": "2021-06-27T07:57:22.348830Z",
     "iopub.status.idle": "2021-06-27T07:57:22.351288Z",
     "shell.execute_reply": "2021-06-27T07:57:22.350728Z",
     "shell.execute_reply.started": "2021-06-26T20:12:54.009576Z"
    },
    "papermill": {
     "duration": 0.082035,
     "end_time": "2021-06-27T07:57:22.351430",
     "exception": false,
     "start_time": "2021-06-27T07:57:22.269395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set len:  198\n",
      "test set len:  22\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(data, test_size = 0.1, random_state = 42)\n",
    "print('train set len: ',len(train_data))\n",
    "print('test set len: ',len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c438114",
   "metadata": {},
   "source": [
    "## Dump the JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cb0fc610",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(train_data, open('data/train_data.json', 'w'))\n",
    "json.dump(test_data, open('data/test_data.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9a4225e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import math\n",
    "import spacy\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from spacy.tokens import DocBin\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "def preprocess_dataturks_json(dataturks_JSON_FilePath):\n",
    "    training_data = []\n",
    "    lines=[]\n",
    "    with open(dataturks_JSON_FilePath, 'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        data = json.loads(line)\n",
    "        text = data['content'].replace(\"\\n\", \" \")\n",
    "        entities = []\n",
    "        data_annotations = data['annotation']\n",
    "        if data_annotations is not None:\n",
    "            for annotation in data_annotations:\n",
    "                point = annotation['points']\n",
    "                labels = annotation['label']\n",
    "                if not isinstance(labels, list):\n",
    "                    labels = [labels]\n",
    "                for label in labels:\n",
    "                    point_start = point['start']\n",
    "                    point_end = point['end']\n",
    "                    point_text = point['text']\n",
    "                    lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
    "                    rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
    "                    if lstrip_diff != 0:\n",
    "                        point_start = point_start + lstrip_diff\n",
    "                    if rstrip_diff != 0:\n",
    "                        point_end = point_end - rstrip_diff\n",
    "                    entities.append((point_start, point_end + 1 , label))\n",
    "            training_data.append((text, {\"entities\" : entities}))\n",
    "    return training_data\n",
    "\n",
    "def trim_entity_spans(data: list) -> list:\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(\n",
    "                    text[valid_start]):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and invalid_span_tokens.match(\n",
    "                    text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            valid_entities.append([valid_start, valid_end, label])\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "    return cleaned_data\n",
    "\n",
    "def get_spacy_doc(file, data):\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    db = DocBin()\n",
    "    for text, annot in tqdm(data):\n",
    "        doc = nlp.make_doc(text)\n",
    "        annot = annot['entities']\n",
    "        ents = []\n",
    "        entity_indices = []\n",
    "        for start, end, label in annot:\n",
    "            skip_entity = False\n",
    "            for idx in range(start, end):\n",
    "                if idx in entity_indices:\n",
    "                    skip_entity = True\n",
    "                    break\n",
    "            if skip_entity == True:\n",
    "                continue\n",
    "            entity_indices = entity_indices + list(range(start, end))\n",
    "            try:\n",
    "                span = doc.char_span(start, end, label = label, alignment_mode = \"strict\")\n",
    "            except:\n",
    "                continue\n",
    "            if span is None:\n",
    "                err_data = str([start, end])+ \" \" + str(text) + \"\\n\"\n",
    "                file.write(err_data)\n",
    "            else:\n",
    "                ents.append(span)\n",
    "        try:\n",
    "            doc.ents = ents\n",
    "            db.add(doc)\n",
    "        except:\n",
    "            pass\n",
    "    return db\n",
    "\n",
    "def train_test_split(data, test_size, random_state):\n",
    "    random.Random(random_state).shuffle(data)\n",
    "    test_idx = len(data) - math.floor(test_size * len(data))\n",
    "    train_set = data[0: test_idx]\n",
    "    test_set = data[test_idx: ]\n",
    "    return train_set, test_set\n",
    "\n",
    "def convert_dataturk_json_to_spacy_data_format(train_data, test_data,spacy_dir):\n",
    "    file = open(\"sample.txt\", \"w\",encoding='utf-8')\n",
    "    db = get_spacy_doc(file, train_data)\n",
    "    train_spacy_path=spacy_dir+'/train_data.spacy'\n",
    "    db.to_disk(train_spacy_path)\n",
    "\n",
    "\n",
    "    db = get_spacy_doc(file, test_data)\n",
    "    test_spacy_path=spacy_dir+'/test_data.spacy'\n",
    "    db.to_disk(test_spacy_path)\n",
    "    file.close()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    dataset_path = \"data/jsons/Entity Recognition in Resumes.json\"\n",
    "    spacy_dir='data/spacy_format'\n",
    "    dataset=trim_entity_spans(preprocess_dataturks_json(dataset_path))\n",
    "    train_data, test_data = train_test_split(dataset, test_size = 0.1, random_state = 42)\n",
    "    convert_dataturk_json_to_spacy_data_format(train_data, test_data,spacy_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "458419ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32e94d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 198/198 [00:02<00:00, 75.68it/s] \n",
      "100%|██████████| 22/22 [00:00<00:00, 28.71it/s]\n"
     ]
    }
   ],
   "source": [
    "file = open(\"error.txt\", \"w\")\n",
    "db = get_spacy_doc(file, train_data)\n",
    "db.to_disk(\"spacy_data/train_data.spacy\")\n",
    "\n",
    "db = get_spacy_doc(file, test_data)\n",
    "db.to_disk(\"spacy_data/test_data.spacy\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef6956c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m spacy train config.cfg --output ./output --paths.train spacy_data/train_data.spacy --paths.dev spacy_data/test_data.spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de57362c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harini Komaravelli -->>>> Name\n",
      "Test Analyst -->>>> Designation\n",
      "Oracle -->>>> Companies worked at\n",
      "Hyderabad -->>>> Location\n",
      "Hyderabad -->>>> Location\n",
      "6 Yrs -->>>> Years of Experience\n",
      "Manual and Automation testing. -->>>> Designation\n",
      "QA Analyst -->>>> Designation\n",
      "Oracle -->>>> Companies worked at\n",
      "Test Analyst -->>>> Designation\n",
      "Oracle -->>>> Companies worked at\n",
      "Hyderabad -->>>> Location\n",
      "Infosys Ltd -->>>> Companies worked at\n",
      "Hyderabad -->>>> Location\n",
      "Hyderabad -->>>> Location\n",
      "Test Analyst -->>>> Designation\n",
      "Oracle -->>>> Companies worked at\n",
      "Hyderabad -->>>> Location\n",
      "QA Analyst -->>>> Designation\n",
      "6 years -->>>> Years of Experience\n",
      "Oracle -->>>> Companies worked at\n",
      "MCA -->>>> Degree\n",
      "Osmania University -->>>> College Name\n",
      "B.Sc. in Computer Science -->>>> Degree\n",
      "Osmania University -->>>> College Name\n",
      "Functional Testing, Blue Prism, Qtp -->>>> Skills\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"output/model-best\")\n",
    "\n",
    "doc = nlp(\"Harini Komaravelli Test Analyst at Oracle, Hyderabad  Hyderabad, Telangana - Email me on Indeed: indeed.com/r/Harini- Komaravelli/2659eee82e435d1b  ➢ 6 Yrs. of IT Experience in Manual and Automation testing.  WORK EXPERIENCE  QA Analyst  Oracle  Test Analyst at Oracle, Hyderabad  Infosys Ltd -  Hyderabad, Telangana -  November 2011 to February 2016  Hyderabad from Nov 2011 to Feb17 2016 ➢ Worked in Tata Consultancy Services, Hyderabad from Feb 24 to Apr 11 2017 ➢ Currently working as a Test Analyst at Oracle, Hyderabad  QA Analyst with 6 years of IT experience  Oracle  EDUCATION  MCA  Osmania University  B.Sc. in Computer Science  Osmania University  SKILLS  Functional Testing, Blue Prism, Qtp  ADDITIONAL INFORMATION  Area of Expertise:  ➢ Familiar with Agile Methodologies. ➢ Having knowledge in Energy (Petroleum) & Health Care domains. ➢ Involved in preparation of Test Scenarios. ➢ Preparing Test Data for the test cases.  https://www.indeed.com/r/Harini-Komaravelli/2659eee82e435d1b?isid=rex-download&ikw=download-top&co=IN https://www.indeed.com/r/Harini-Komaravelli/2659eee82e435d1b?isid=rex-download&ikw=download-top&co=IN   ➢ Experienced in development and execution of Test cases effectively. ➢ Experienced in Functional testing, GUI testing, Smoke testing, Regression testing and Integration Testing ➢ Experienced in doing Accessibility testing of an application ➢ Ability to understand user Requirements, Functional and Design specifications. ➢ Good knowledge of SDLC and STLC processes. ➢ Deciding the Severity and Priority of bugs. ➢ Experience in using Microsoft Test Manager & Oracle Test Manager as Test Management Tools. ➢ Having good experience in testing windows based & web based applications. ➢ Involved in Client Interactions for reviews, issues and for any clarifications. ➢ Web Services Testing ➢ Writing Test Scripts in QTP, Testcomplete. ➢ Creating Object Repositories and Function Libraries in QTP. ➢ Enhanced QTP scripts using VB Script. ➢ Strong experience in working with Blue Prism tool ➢ Worked on different Environments like Windows Application & Web Application  Technical Skills:  ❑ Test Automation Tools: Blue Prism, QTP 10.0, Testcomplete ❑ Test Management Tool: Microsoft Test Manager, Oracle Test Manager & JIRA ❑ Databases: Oracle 10g, SQL Server.  ❑ Operating Systems: Windows 7  Project 1: Title: Cadence Client: Baker Hughes  Technologies: Microsoft Visual Studio and Microsoft Team Foundation Server  Client Background: An oilfield services company delivering focused efforts on shale gas and other oilfield services. It provides services, tools and software for drilling and formation evaluation, well completion, production management, seismic data collection and interpretation.  Project Description: AUT (Application under test) is the next generation revolutionary, robust, easy to use scalable well site data acquisition processing and interpretation system for Client's Drilling Services to deliver services that meets cross divisional business requirements consistently.  Project 2:  Description: Paragon supports your entire care team with one tool that your clinicians need to help deliver the best patient care. Designed by physicians, nurses, pharmacists and mid level providers that have a first-hand understanding of clinical workflow needs, Paragon clinical applications allow your caregivers to focus on what matters most; spending time caring for patients. Since Paragon is fully-integrated across all applications and built around a single patient database, information    entered anywhere in the system is immediately available to the entire care team. Immediate access not only helps clinicians make better treatment decisions - it also helps promote patient safety. Paragon offers a broad suite of multidisciplinary clinical software solutions together with anytime, anywhere access to the complete patient record.  Responsibilities:  • Performed Smoke testing and Regression testing. • Involved in Generating and Executing Test Script using Quick Test Pro & Blue Prism • Usability and User Interface Testing. • Involved in Defect tracking and reporting the bugs using TFS • Participated in frequent walk-through meetings with Internal Quality Assurance groups and with development groups. • Participated in client calls and clarifying the doubts by having AT&T sessions • Involved in functional, regression and smoke testing to validate the application data changes done in windows application • Certifying the build status by running the scripts as part of smoke testing  Project 3:  Description: Food & Beverages R&A: Easily manage business across multiple locations while reducing IT cost and complexity. Cloud-based point-of-sale (POS) solutions enable centralized enterprise management with lower upfront costs and a smaller footprint.  Responsibilities:  • Performed Functional testing and Regression testing. • Involved in Generating and Executing Test Scripts using Blue Prism tool and Open script • Involved in preparing bots using Blue Prism tool. • Accessibility testing of the web application • Involved in Defect tracking and reporting the bugs using JIRA • WebServices testing by calling API's to export the data\",)\n",
    "for ents in doc.ents:\n",
    "    print(ents.text, \"-->>>>\", ents.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 363.96657,
   "end_time": "2021-06-27T08:01:53.067747",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-27T07:55:49.101177",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
